{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mini Intro to Textual Analysis: Wordlist\n",
    "by Dr Liang Jin\n",
    "\n",
    "Part of Mini Python Sessions: [github.com/drliangjin/minipy](https://github.com/drliangjin/minipy)\n",
    "\n",
    "Bodnaruk, Loughran, and McDonald (2015)\n",
    "\n",
    "Note: Predefined SEC strings\n",
    "- 10K filing codes: '10-K', '10-K405', '10KSB', '10-KSB', '10KSB40'\n",
    "- 10Q filing codes: '10-Q', '10QSB', '10-QSB'\n",
    "\n",
    "Excellent (but outdated) resources by Bill McDonald can be found from [Software Repository for Accounting and Finance](https://sraf.nd.edu/). It's relevant but dangerous to use those codes straightway."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# NLTK stands for Natural Language Took Kits\n",
    "# It is the most popular advanced textual analysis tool/package in Python\n",
    "# It has tons of features and also comes with a large collection of corpus (a collection of text) to play with\n",
    "# These features however need to be downloaded by running the following:\n",
    "# import nltk\n",
    "# nltk.download(); This thing is huge, please note it will take quite sometime to finalise the downloads."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting Text Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apple's 10-K filing on 2017\n",
    "# IBM's 10-K filing on 20120228\n",
    "urls = ['https://www.sec.gov/Archives/edgar/data/51143/000104746912001742/a2206744z10-k.htm',\n",
    "        'https://www.sec.gov/Archives/edgar/data/320193/000032019317000070/a10-k20179302017.htm']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Soup, HTML and Text\n",
    "- Remove tables: All characters appearing between `<TABLE>` and `</TABLE>` tages are removed \n",
    "- NOTE: unless numeric characters/(alphabetic + numeric chars) <= 15% (BLM, 2015), **can you do this?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to create our Soup object and then extract text\n",
    "# The key is here is: when we have HTML structure, we remove tables otherwise it can be tricky\n",
    "def url_to_text(url):\n",
    "    resp = requests.get(url)\n",
    "    soup = BeautifulSoup(resp.text, 'lxml')\n",
    "    for table in soup.find_all('table'):\n",
    "        table.decompose()\n",
    "    text = soup.get_text()\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Actually obtain the text from requests via EDGAR, parsing using BS4 then text\n",
    "texts = [url_to_text(url) for url in urls]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Store our data within Python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hold files locally\n",
    "import pickle # nice module name, isn't it?\n",
    "\n",
    "firms = ['Apple', 'IBM']\n",
    "\n",
    "# access index and value for a list\n",
    "for idx, val in enumerate(firms):\n",
    "    with open(val + \".pkl\", \"wb\") as f:\n",
    "        pickle.dump(texts[idx], f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load our pickled data into a dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load pickled files into a dictionary\n",
    "# Key: company name\n",
    "# value: parsed 10K text\n",
    "data = {}\n",
    "\n",
    "for _, val in enumerate(firms):\n",
    "    with open(val + \".pkl\", \"rb\") as f:\n",
    "        data[val] = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['Apple', 'IBM'])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check company names to make sure our data has been loaded properly\n",
    "data.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n10-K\\n1\\na10-k20179302017.htm\\n10-K\\n\\n\\n\\n\\nDocument\\nUNITED STATESSECURITIES AND EXCHANGE COMMISSIONWashington, D.C. 20549FORM 10-K(Mark One)☒ ANNUAL REPORT PURSUANT TO SECTION 13 OR 15(d) OF THE SECURITIES EXCHANGE ACT OF 1934For the fiscal year ended September\\xa030, 2017or☐ TRANSITION REPORT PURSUANT TO SECTION 13 OR 15(d) OF THE SECURITIES EXCHANGE ACT OF 1934For the transition period from\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 to \\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0Commission File Number: 001-36743Apple Inc.(Exact name of Registrant as specified in its charter)(408) 996-1010(Registrant’s telephone number, including area code)Securities registered pursuant to Section\\xa012(b)\\xa0of the Act:Securities registered pursuant to Section\\xa012(g)\\xa0of the Act:  NoneIndicate by check mark if the Registrant is a well-known seasoned issuer, as defined in Rule 405 of the Securities Act.Yes\\xa0\\xa0☒\\xa0\\xa0\\xa0\\xa0No\\xa0\\xa0☐Indicate by check mark if the Registrant is not required to file reports pursuant to Section\\xa013 or Section\\xa015(d) of the Act.Yes\\xa0\\xa0☐\\xa0\\xa0\\xa0\\xa0No\\xa0\\xa0☒Indicate by check mark whether the Registrant (1)\\xa0has filed all reports required to be filed by Section\\xa013 or 15(d)\\xa0of the Securities Exchange Act of 1934 during the preceding 12 months (or for such shorter period that the Registrant was required to file such reports), and (2)\\xa0has been subject to such filing requirements for the past 90 days.Yes\\xa0\\xa0☒\\xa0\\xa0\\xa0\\xa0No\\xa0\\xa0☐Indicate by check mark whether the Registrant has submitted electronically and posted on its corporate Web site, if any, every Interactive Data File required to be submitted and posted pursuant to Rule 405 of Regulation S-T (§232.405 of this chapter) during the preceding 12 months (or for such shorter period that the Registrant was required to submit and post such files).Yes\\xa0\\xa0☒\\xa0\\xa0\\xa0\\xa0No\\xa0\\xa0☐Indicate by check mark if disclosure of delinquent filers pursuant to Item\\xa0405 of Regulation S-K (§229.405 of this chapter) is not contained herein, and will not be contained, to the best of the Registrant’s knowledge, in definitive proxy or information statements incorporate'"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check texts\n",
    "#\n",
    "data['IBM'][:2000]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (very) Basic Features Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's first define some handy funcs\n",
    "# Count total Words\n",
    "def count_words(text):\n",
    "    return len(str(text).split(\" \"))\n",
    "\n",
    "# Count total characters\n",
    "def count_chars(text):\n",
    "    return len(str(text))\n",
    "\n",
    "# Count numerics\n",
    "def count_digit(text):\n",
    "    return len([word for word in str(text).split(\" \") if word.isdigit()]) # isdigit() is a string method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count stopwords\n",
    "def count_stopwords(text, stop=stopwords.words('english')):\n",
    "    num = len([word for word in str(text).split(' ') if word in stop])\n",
    "    perc = num/len(str(text).split(\" \"))\n",
    "    return num, perc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# stopwords list from LM's website\n",
    "\n",
    "lm_list = ['ME', 'MY', 'MYSELF', 'WE', 'OUR', 'OURS', 'OURSELVES', 'YOU', 'YOUR', 'YOURS',\n",
    "                       'YOURSELF', 'YOURSELVES', 'HE', 'HIM', 'HIS', 'HIMSELF', 'SHE', 'HER', 'HERS', 'HERSELF',\n",
    "                       'IT', 'ITS', 'ITSELF', 'THEY', 'THEM', 'THEIR', 'THEIRS', 'THEMSELVES', 'WHAT', 'WHICH',\n",
    "                       'WHO', 'WHOM', 'THIS', 'THAT', 'THESE', 'THOSE', 'AM', 'IS', 'ARE', 'WAS', 'WERE', 'BE',\n",
    "                       'BEEN', 'BEING', 'HAVE', 'HAS', 'HAD', 'HAVING', 'DO', 'DOES', 'DID', 'DOING', 'AN',\n",
    "                       'THE', 'AND', 'BUT', 'IF', 'OR', 'BECAUSE', 'AS', 'UNTIL', 'WHILE', 'OF', 'AT', 'BY',\n",
    "                       'FOR', 'WITH', 'ABOUT', 'BETWEEN', 'INTO', 'THROUGH', 'DURING', 'BEFORE',\n",
    "                       'AFTER', 'ABOVE', 'BELOW', 'TO', 'FROM', 'UP', 'DOWN', 'IN', 'OUT', 'ON', 'OFF', 'OVER',\n",
    "                       'UNDER', 'AGAIN', 'FURTHER', 'THEN', 'ONCE', 'HERE', 'THERE', 'WHEN', 'WHERE', 'WHY',\n",
    "                       'HOW', 'ALL', 'ANY', 'BOTH', 'EACH', 'FEW', 'MORE', 'MOST', 'OTHER', 'SOME', 'SUCH',\n",
    "                       'NO', 'NOR', 'NOT', 'ONLY', 'OWN', 'SAME', 'SO', 'THAN', 'TOO', 'VERY', 'CAN',\n",
    "                       'JUST', 'SHOULD', 'NOW']\n",
    "\n",
    "lm_stopwords = [word.lower() for word in lm_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(36277, (12611, 0.3476307302147366))"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# A very large proportion of the whole text is stopwords!\n",
    "count_words(data['IBM']), count_stopwords(data['IBM'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load our dictionary into Pandas DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can either keep it in dictionary format or put it into a pandas dataframe\n",
    "# put our corpus into a pandas dataframe\n",
    "import pandas as pd\n",
    "pd.set_option('max_colwidth', 150)\n",
    "\n",
    "df = pd.DataFrame.from_dict(data, orient='index', columns = ['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Apple</th>\n",
       "      <td>\\n10-K\\n1\\na2206744z10-k.htm\\n10-K\\n\\n\\nQuickLinks\\n -- Click here to rapidly navigate through this document\\n\\n\\n\\n\\n\\n\\n\\n\\n \\n\\n \\nUNITED STATE...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>IBM</th>\n",
       "      <td>\\n10-K\\n1\\na10-k20179302017.htm\\n10-K\\n\\n\\n\\n\\nDocument\\nUNITED STATESSECURITIES AND EXCHANGE COMMISSIONWashington, D.C. 20549FORM 10-K(Mark One)☒...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                                        text\n",
       "Apple  \\n10-K\\n1\\na2206744z10-k.htm\\n10-K\\n\\n\\nQuickLinks\\n -- Click here to rapidly navigate through this document\\n\\n\\n\\n\\n\\n\\n\\n\\n \\n\\n \\nUNITED STATE...\n",
       "IBM    \\n10-K\\n1\\na10-k20179302017.htm\\n10-K\\n\\n\\n\\n\\nDocument\\nUNITED STATESSECURITIES AND EXCHANGE COMMISSIONWashington, D.C. 20549FORM 10-K(Mark One)☒..."
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleaning the Text Data (Text Pre-Processing)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data analysis is more of art than science, in a sense. For example, when we are trying to clean our data, we need manual inputs and our judgements. After all, no data can be perfect; especially for text data, the cleaning or pre-processing can go on forever. We are just going to execute the most common/simple cleaning steps; you can continue to work on to improve your results, i.e., replicating BLM(2015)'s work..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Common data cleanning steps:\n",
    "- Make text all lower case\n",
    "- Remove special expression from non-human languages\n",
    "- Remove punctuation\n",
    "- Remove numerical values\n",
    "- Remove stop words\n",
    "- Tokenize text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### More advanced cleaning steps after tokenization:\n",
    "- Stemming / lemmatization\n",
    "- Tagging\n",
    "- N-grams\n",
    "- And more..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Replicate BLM(2015) on constraining words\n",
    "- Read the paper carefully, focusing on sections such as `II. Data`\n",
    "- Go through `Appendix B. Parsing the 10-K Filings` (discard the first 4 steps for now as they are for txt files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# OK, basic text cleaning\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# deal with reserved special html characters such as non-breaking space (`&nbsp`)\n",
    "html_chars = {'&lt': 'lt', '&#60': 'lt', \n",
    "              '&gt': 'gt', '&#62': 'gt',\n",
    "              '&nbsp': '', '&#160': '', \n",
    "              '&quot': '\"', '&#34': '\"', \n",
    "              '&apos': '\\'', '&#39': '\\'',\n",
    "              '&amp': '&', '&#38': '&'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text_round1(text):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text_round2(text):\n",
    "    # convert to lower case\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'(\\t|\\v)', '', text)\n",
    "    # remove \\xa0 which is non-breaking space from ISO 8859-1, how to delete all remaining ISO 8859-1 symbols & chars?\n",
    "    text = re.sub(r'\\xa0', ' ', text)\n",
    "    # remove newline feeds (\\n) following hyphens\n",
    "    text = re.sub(r'(-+)\\n{2,}', r'\\1', text)\n",
    "    # remove hyphens preceded and followed by a blank space\n",
    "    text = re.sub(r'\\s-\\s', '', text)\n",
    "    # replace 'and/or' with 'and or'\n",
    "    text = re.sub(r'and/or', r'and or', text)\n",
    "    # tow or more hypens, periods, or equal signs, possiblly followed by spaces are removed\n",
    "    text = re.sub(r'[-|\\.|=]{2,}\\s*', r'', text)\n",
    "    # all underscores are removed\n",
    "    text = re.sub(r'_', '', text)\n",
    "    # 3 or more spaces are replaced by a single space\n",
    "    text = re.sub(r'\\s{3,}', ' ', text)\n",
    "    # three or more line feeds, possibly separated by spaces are replaced by two line feeds\n",
    "    text = re.sub(r'(\\n\\s*){3,}', '\\n\\n', text)\n",
    "    # remove hyphens before a line feed\n",
    "    text = re.sub(r'-+\\n', '\\n', text)\n",
    "    # replace hyphens preceding a capitalized letter with a space\n",
    "    text = re.sub(r'-+([A-Z].*)', r' \\1', text)\n",
    "    # remove capitalized or all capitals for March, May and August\n",
    "    text = re.sub(r'(March|MARCH|May|MAY|August|AUGUST)', '', text)\n",
    "    # remove punctuations\n",
    "    # text = re.sub('[]'.format(re.escape(string.punctuation)), '', text)\n",
    "    # remove line feeds\n",
    "    # text = re.sub('\\n', ' ', text)\n",
    "    # remove numbers?\n",
    "    # replace single line feed \\n with single space\n",
    "    #text = re.sub(r'\\n', ' ', text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Apple</th>\n",
       "      <td>\\n10-K\\n1\\na2206744z10-k.htm\\n10-K\\n\\n\\nQuickLinks\\n -- Click here to rapidly navigate through this document\\n\\n\\n\\n\\n\\n\\n\\n\\n \\n\\n \\nUNITED STATE...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>IBM</th>\n",
       "      <td>\\n10-K\\n1\\na10-k20179302017.htm\\n10-K\\n\\n\\n\\n\\nDocument\\nUNITED STATESSECURITIES AND EXCHANGE COMMISSIONWashington, D.C. 20549FORM 10-K(Mark One)☒...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                                        text\n",
       "Apple  \\n10-K\\n1\\na2206744z10-k.htm\\n10-K\\n\\n\\nQuickLinks\\n -- Click here to rapidly navigate through this document\\n\\n\\n\\n\\n\\n\\n\\n\\n \\n\\n \\nUNITED STATE...\n",
       "IBM    \\n10-K\\n1\\na10-k20179302017.htm\\n10-K\\n\\n\\n\\n\\nDocument\\nUNITED STATESSECURITIES AND EXCHANGE COMMISSIONWashington, D.C. 20549FORM 10-K(Mark One)☒..."
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.DataFrame(df['text'].apply(clean_text_round2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Apple</th>\n",
       "      <td>\\n10-k\\n1\\na2206744z10-k.htm\\n10-k quicklinks\\n click here to rapidly navigate through this document united states\\nsecurities and exchange commis...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>IBM</th>\n",
       "      <td>\\n10-k\\n1\\na10-k20179302017.htm\\n10-k document\\nunited statessecurities and exchange commissionwashington, d.c. 20549form 10-k(mark one)☒ annual r...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                                        text\n",
       "Apple  \\n10-k\\n1\\na2206744z10-k.htm\\n10-k quicklinks\\n click here to rapidly navigate through this document united states\\nsecurities and exchange commis...\n",
       "IBM    \\n10-k\\n1\\na10-k20179302017.htm\\n10-k document\\nunited statessecurities and exchange commissionwashington, d.c. 20549form 10-k(mark one)☒ annual r..."
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Advanced Text Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lm_constrwords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Document-Term Matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To continue working on our textual analysis of 10-K filings (can be as simple as word counts or can be as fancy as machine learning based techniques, the text must be tokenized, meaning broken down into smaller pieces. NLTK provides methods to do so, such as breaking text into sentenses and words. We can also do this using scikit-learn's CountVectorizer. The output will be multiple rows representing different documents (such a 10-K file) and multiple columns (lots of columns) representing a different word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We are going to create a document-term matrix using CountVectorizer\n",
    "# NOTE: we can remove stop words which are common words that add no additional meaning to the text, such as 'a', 'the', etc.\n",
    "# NOTE: later we can try use LM defined stopwords for 10-K, we can even create our own stopwords dictionary\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "cv = CountVectorizer(stop_words='english')\n",
    "cv_data = cv.fit_transform(df.text)\n",
    "\n",
    "dtm = pd.DataFrame(cv_data.toarray(), columns=cv.get_feature_names())\n",
    "dtm.index = df.index\n",
    "dtm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's pickle our dataframes\n",
    "dtm.to_pickle('dtm.pkl')\n",
    "\n",
    "# and our CountVectorizer object\n",
    "with open(\"cv.pkl\", \"wb\") as f:\n",
    "    pickle.dump(cv, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Top words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = dtm.transpose()\n",
    "data.head(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the most common words used by the 10-K files\n",
    "top_words = {}\n",
    "\n",
    "for firm in data.columns:\n",
    "    top = data[firm].sort_values(ascending=False).head(30)\n",
    "    top_words[firm] = list(zip(top.index, top.values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bag of Words (BoW)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a bag of contrainning words from BLM (2015)\n",
    "lm_constrwords = []\n",
    "\n",
    "with open('words_from_pdf.txt', 'r') as rf:\n",
    "        lines = rf.read().splitlines() # readlines() create a newline character \"\\n\" each line\n",
    "        for line in lines:\n",
    "            words = line.split(sep=' ')\n",
    "            for word in words:\n",
    "                lm_constrwords.append(word)\n",
    "\n",
    "# sort words alphabetically               \n",
    "lm_constrwords.sort()\n",
    "\n",
    "# You can write to a local file of course    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['abide',\n",
       " 'abiding',\n",
       " 'bound',\n",
       " 'bounded',\n",
       " 'commit',\n",
       " 'commitment',\n",
       " 'commitments',\n",
       " 'commits',\n",
       " 'committed',\n",
       " 'committing',\n",
       " 'compel',\n",
       " 'compelled',\n",
       " 'compelling',\n",
       " 'compels',\n",
       " 'comply',\n",
       " 'compulsion',\n",
       " 'compulsory',\n",
       " 'confine',\n",
       " 'confined',\n",
       " 'confinement',\n",
       " 'confines',\n",
       " 'confining',\n",
       " 'constrain',\n",
       " 'constrained',\n",
       " 'constraining',\n",
       " 'constrains',\n",
       " 'constraint',\n",
       " 'constraints',\n",
       " 'covenant',\n",
       " 'covenanted',\n",
       " 'covenanting',\n",
       " 'covenants',\n",
       " 'depend',\n",
       " 'dependance',\n",
       " 'dependances',\n",
       " 'dependant',\n",
       " 'dependencies',\n",
       " 'dependent',\n",
       " 'depending',\n",
       " 'depends',\n",
       " 'dictate',\n",
       " 'dictated',\n",
       " 'dictates',\n",
       " 'dictating',\n",
       " 'directive',\n",
       " 'directives',\n",
       " 'earmark',\n",
       " 'earmarked',\n",
       " 'earmarking',\n",
       " 'earmarks',\n",
       " 'encumber',\n",
       " 'encumbered',\n",
       " 'encumbering',\n",
       " 'encumbers',\n",
       " 'encumbrance',\n",
       " 'encumbrances',\n",
       " 'entail',\n",
       " 'entailed',\n",
       " 'entailing',\n",
       " 'entails',\n",
       " 'entrench',\n",
       " 'entrenched',\n",
       " 'escrow',\n",
       " 'escrowed',\n",
       " 'escrows',\n",
       " 'forbade',\n",
       " 'forbid',\n",
       " 'forbidden',\n",
       " 'forbidding',\n",
       " 'forbids',\n",
       " 'impair',\n",
       " 'impaired',\n",
       " 'impairing',\n",
       " 'impairment',\n",
       " 'impairments',\n",
       " 'impairs',\n",
       " 'impose',\n",
       " 'imposed',\n",
       " 'imposes',\n",
       " 'imposing',\n",
       " 'imposition',\n",
       " 'impositions',\n",
       " 'indebted',\n",
       " 'inhibit',\n",
       " 'inhibited',\n",
       " 'inhibiting',\n",
       " 'inhibits',\n",
       " 'insist',\n",
       " 'insisted',\n",
       " 'insistence',\n",
       " 'insisting',\n",
       " 'insists',\n",
       " 'irrevocable',\n",
       " 'irrevocably',\n",
       " 'limit',\n",
       " 'limiting',\n",
       " 'limits',\n",
       " 'mandate',\n",
       " 'mandated',\n",
       " 'mandates',\n",
       " 'mandating',\n",
       " 'mandatory',\n",
       " 'manditorily',\n",
       " 'necessitate',\n",
       " 'necessitated',\n",
       " 'necessitates',\n",
       " 'necessitating',\n",
       " 'noncancelable',\n",
       " 'noncancellable',\n",
       " 'obligate',\n",
       " 'obligated',\n",
       " 'obligates',\n",
       " 'obligating',\n",
       " 'obligation',\n",
       " 'obligations',\n",
       " 'obligatory',\n",
       " 'oblige',\n",
       " 'obliged',\n",
       " 'obliges',\n",
       " 'permissible',\n",
       " 'permission',\n",
       " 'permissions',\n",
       " 'permitted',\n",
       " 'permitting',\n",
       " 'pledge',\n",
       " 'pledged',\n",
       " 'pledges',\n",
       " 'pledging',\n",
       " 'preclude',\n",
       " 'precluded',\n",
       " 'precludes',\n",
       " 'precluding',\n",
       " 'precondition',\n",
       " 'preconditions',\n",
       " 'preset',\n",
       " 'prevent',\n",
       " 'prevented',\n",
       " 'preventing',\n",
       " 'prevents',\n",
       " 'prohibit',\n",
       " 'prohibited',\n",
       " 'prohibiting',\n",
       " 'prohibition',\n",
       " 'prohibitions',\n",
       " 'prohibitive',\n",
       " 'prohibitively',\n",
       " 'prohibitory',\n",
       " 'prohibits',\n",
       " 'refrain',\n",
       " 'refraining',\n",
       " 'refrains',\n",
       " 'require',\n",
       " 'required',\n",
       " 'requirement',\n",
       " 'requirements',\n",
       " 'requires',\n",
       " 'requiring',\n",
       " 'restrain',\n",
       " 'restrained',\n",
       " 'restraining',\n",
       " 'restrains',\n",
       " 'restraint',\n",
       " 'restraints',\n",
       " 'restrict',\n",
       " 'restricted',\n",
       " 'restricting',\n",
       " 'restriction',\n",
       " 'restrictions',\n",
       " 'restrictive',\n",
       " 'restrictively',\n",
       " 'restrictiveness',\n",
       " 'restricts',\n",
       " 'stipulate',\n",
       " 'stipulated',\n",
       " 'stipulates',\n",
       " 'stipulating',\n",
       " 'stipulation',\n",
       " 'stipulations',\n",
       " 'strict',\n",
       " 'stricter',\n",
       " 'strictest',\n",
       " 'strictly',\n",
       " 'unavailability',\n",
       " 'unavailable']"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lm_constrwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
