{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mini Intro to Textual Analysis: Wordlist\n",
    "by Dr Liang Jin\n",
    "\n",
    "Part of Mini Python Sessions: [github.com/drliangjin/minipy](https://github.com/drliangjin/minipy)\n",
    "\n",
    "Bodnaruk, Loughran, and McDonald (2015)\n",
    "\n",
    "Note: Predefined SEC strings\n",
    "- 10K filing codes: '10-K', '10-K405', '10KSB', '10-KSB', '10KSB40'\n",
    "- 10Q filing codes: '10-Q', '10QSB', '10-QSB'\n",
    "\n",
    "Excellent (but outdated) resources by Bill McDonald can be found from [Software Repository for Accounting and Finance](https://sraf.nd.edu/). It's relevant but dangerous to use those codes straightway."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# NLTK stands for Natural Language Took Kits\n",
    "# It is the most popular advanced textual analysis tool/package in Python\n",
    "# It has tons of features and also comes with a large collection of corpus (a collection of text) to play with\n",
    "# These features however need to be downloaded by running the following:\n",
    "import nltk\n",
    "nltk.download(); #This thing is huge, please note it will take quite sometime to finalise the downloads."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting Text Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apple's 10-K filing on 2017\n",
    "# IBM's 10-K filing on 20120228\n",
    "urls = ['https://www.sec.gov/Archives/edgar/data/51143/000104746912001742/a2206744z10-k.htm',\n",
    "        'https://www.sec.gov/Archives/edgar/data/320193/000032019317000070/a10-k20179302017.htm']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Soup, HTML and Text\n",
    "- Remove tables: All characters appearing between `<TABLE>` and `</TABLE>` tages are removed \n",
    "- NOTE: unless numeric characters/(alphabetic + numeric chars) <= 15% (BLM, 2015), **can you do this?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to create our Soup object and then extract text\n",
    "# The key is here is: when we have HTML structure, we remove tables otherwise it can be tricky\n",
    "def url_to_text(url):\n",
    "    resp = requests.get(url)\n",
    "    soup = BeautifulSoup(resp.text, 'html.parser')\n",
    "    for table in soup.find_all('table'):\n",
    "        table.decompose()\n",
    "    text = soup.get_text()\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Actually obtain the text from requests via EDGAR, parsing using BS4 then text\n",
    "texts = [url_to_text(url) for url in urls]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# have a peek!\n",
    "texts[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Store our data within Python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hold files locally\n",
    "import pickle # nice module name, isn't it?\n",
    "\n",
    "firms = ['Apple', 'IBM']\n",
    "\n",
    "# access index and value for a list\n",
    "for idx, val in enumerate(firms):\n",
    "    with open(val + \".pkl\", \"wb\") as f:\n",
    "        pickle.dump(texts[idx], f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load our pickled data into a dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load pickled files into a dictionary\n",
    "# Key: company name\n",
    "# value: parsed 10K text\n",
    "data = {}\n",
    "\n",
    "for _, val in enumerate(firms):\n",
    "    with open(val + \".pkl\", \"rb\") as f:\n",
    "        data[val] = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check company names to make sure our data has been loaded properly\n",
    "data.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check texts\n",
    "#\n",
    "data['IBM'][:2000]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (very) Basic Features Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's first define some handy funcs\n",
    "# Count total Words\n",
    "def count_words(text):\n",
    "    return len(str(text).split(\" \"))\n",
    "\n",
    "# Count total characters\n",
    "def count_chars(text):\n",
    "    return len(str(text))\n",
    "\n",
    "# Count numerics\n",
    "def count_digit(text):\n",
    "    return len([word for word in str(text).split(\" \") if word.isdigit()]) # isdigit() is a string method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stopwords are meaningless...\n",
    "# NOTE: you need to run `nltk.download()` and download required nltk packages to access this feature\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count stopwords\n",
    "def count_stopwords(text, stop=stopwords.words('english')):\n",
    "    num = len([word for word in str(text).split(' ') if word in stop])\n",
    "    perc = num/len(str(text).split(\" \"))\n",
    "    return num, perc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# stopwords list from LM's website\n",
    "\n",
    "lm_list = ['ME', 'MY', 'MYSELF', 'WE', 'OUR', 'OURS', 'OURSELVES', 'YOU', 'YOUR', 'YOURS',\n",
    "                       'YOURSELF', 'YOURSELVES', 'HE', 'HIM', 'HIS', 'HIMSELF', 'SHE', 'HER', 'HERS', 'HERSELF',\n",
    "                       'IT', 'ITS', 'ITSELF', 'THEY', 'THEM', 'THEIR', 'THEIRS', 'THEMSELVES', 'WHAT', 'WHICH',\n",
    "                       'WHO', 'WHOM', 'THIS', 'THAT', 'THESE', 'THOSE', 'AM', 'IS', 'ARE', 'WAS', 'WERE', 'BE',\n",
    "                       'BEEN', 'BEING', 'HAVE', 'HAS', 'HAD', 'HAVING', 'DO', 'DOES', 'DID', 'DOING', 'AN',\n",
    "                       'THE', 'AND', 'BUT', 'IF', 'OR', 'BECAUSE', 'AS', 'UNTIL', 'WHILE', 'OF', 'AT', 'BY',\n",
    "                       'FOR', 'WITH', 'ABOUT', 'BETWEEN', 'INTO', 'THROUGH', 'DURING', 'BEFORE',\n",
    "                       'AFTER', 'ABOVE', 'BELOW', 'TO', 'FROM', 'UP', 'DOWN', 'IN', 'OUT', 'ON', 'OFF', 'OVER',\n",
    "                       'UNDER', 'AGAIN', 'FURTHER', 'THEN', 'ONCE', 'HERE', 'THERE', 'WHEN', 'WHERE', 'WHY',\n",
    "                       'HOW', 'ALL', 'ANY', 'BOTH', 'EACH', 'FEW', 'MORE', 'MOST', 'OTHER', 'SOME', 'SUCH',\n",
    "                       'NO', 'NOR', 'NOT', 'ONLY', 'OWN', 'SAME', 'SO', 'THAN', 'TOO', 'VERY', 'CAN',\n",
    "                       'JUST', 'SHOULD', 'NOW']\n",
    "\n",
    "lm_stopwords = [word.lower() for word in lm_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# A very large proportion of the whole text is stopwords!\n",
    "count_stopwords(data['IBM'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# or try out LM stopwords\n",
    "count_stopwords(data['IBM'], stop=lm_stopwords)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load our dictionary into Pandas DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can either keep it in dictionary format or put it into a pandas dataframe\n",
    "# put our corpus into a pandas dataframe\n",
    "import pandas as pd\n",
    "pd.set_option('max_colwidth', 150)\n",
    "\n",
    "df = pd.DataFrame.from_dict(data, orient='index', columns = ['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleaning the Text Data (Text Pre-Processing)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data analysis is more of art than science, in a sense. For example, when we are trying to clean our data, we need manual inputs and our judgements. After all, no data can be perfect; especially for text data, the cleaning or pre-processing can go on forever. We are just going to execute the most common/simple cleaning steps; you can continue to work on to improve your results, i.e., replicating BLM(2015)'s work..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Common data cleanning steps:\n",
    "- Make text all lower case\n",
    "- Remove special expression from non-human languages\n",
    "- Remove punctuation\n",
    "- Remove numerical values\n",
    "- Remove stop words\n",
    "- Tokenize text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### More advanced cleaning steps after tokenization:\n",
    "- Stemming / lemmatization\n",
    "- Tagging\n",
    "- N-grams\n",
    "- And more..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Replicate BLM(2015) on constraining words\n",
    "- Read the paper carefully, focusing on sections such as `II. Data`\n",
    "- Go through `Appendix B. Parsing the 10-K Filings` (discard the first 4 steps for now as they are for txt files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# OK, basic text cleaning\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# deal with reserved special html characters such as non-breaking space (`&nbsp`)\n",
    "html_chars = {'&lt': 'lt', '&#60': 'lt', \n",
    "              '&gt': 'gt', '&#62': 'gt',\n",
    "              '&nbsp': '', '&#160': '', \n",
    "              '&quot': '\"', '&#34': '\"', \n",
    "              '&apos': '\\'', '&#39': '\\'',\n",
    "              '&amp': '&', '&#38': '&'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text_round1(text):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text_round2(text):\n",
    "    # convert to lower case\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'(\\t|\\v)', '', text)\n",
    "    # remove \\xa0 which is non-breaking space from ISO 8859-1, how to delete all remaining ISO 8859-1 symbols & chars?\n",
    "    text = re.sub(r'\\xa0', ' ', text)\n",
    "    # remove newline feeds (\\n) following hyphens\n",
    "    text = re.sub(r'(-+)\\n{2,}', r'\\1', text)\n",
    "    # remove hyphens preceded and followed by a blank space\n",
    "    text = re.sub(r'\\s-\\s', '', text)\n",
    "    # replace 'and/or' with 'and or'\n",
    "    text = re.sub(r'and/or', r'and or', text)\n",
    "    # tow or more hypens, periods, or equal signs, possiblly followed by spaces are removed\n",
    "    text = re.sub(r'[-|\\.|=]{2,}\\s*', r'', text)\n",
    "    # all underscores are removed\n",
    "    text = re.sub(r'_', '', text)\n",
    "    # 3 or more spaces are replaced by a single space\n",
    "    text = re.sub(r'\\s{3,}', ' ', text)\n",
    "    # three or more line feeds, possibly separated by spaces are replaced by two line feeds\n",
    "    text = re.sub(r'(\\n\\s*){3,}', '\\n\\n', text)\n",
    "    # remove hyphens before a line feed\n",
    "    text = re.sub(r'-+\\n', '\\n', text)\n",
    "    # replace hyphens preceding a capitalized letter with a space\n",
    "    text = re.sub(r'-+([A-Z].*)', r' \\1', text)\n",
    "    # remove capitalized or all capitals for March, May and August\n",
    "    text = re.sub(r'(March|MARCH|May|MAY|August|AUGUST)', '', text)\n",
    "    # remove punctuations\n",
    "    # text = re.sub('[]'.format(re.escape(string.punctuation)), '', text)\n",
    "    # remove line feeds\n",
    "    # text = re.sub('\\n', ' ', text)\n",
    "    # remove numbers?\n",
    "    # replace single line feed \\n with single space\n",
    "    #text = re.sub(r'\\n', ' ', text)\n",
    "    return text\n",
    "\n",
    "# NOTE: \n",
    "## drop punctuation within numbers for number count\n",
    "    #text = re.sub('(?!=[0-9])(\\.|,)(?=[0-9])', '', text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.DataFrame(df['text'].apply(clean_text_round2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Advanced Text Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Document-Term Matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To continue working on our textual analysis of 10-K filings (can be as simple as word counts or can be as fancy as machine learning based techniques, the text must be tokenized, meaning broken down into smaller pieces. NLTK provides methods to do so, such as breaking text into sentenses and words. We can also do this using scikit-learn's CountVectorizer. The output will be multiple rows representing different documents (such a 10-K file) and multiple columns (lots of columns) representing a different word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We are going to create a document-term matrix using CountVectorizer\n",
    "# NOTE: we can remove stop words which are common words that add no additional meaning to the text, such as 'a', 'the', etc.\n",
    "# NOTE: later we can try use LM defined stopwords for 10-K, we can even create our own stopwords dictionary\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "cv = CountVectorizer(stop_words='english')\n",
    "cv_data = cv.fit_transform(df.text)\n",
    "\n",
    "dtm = pd.DataFrame(cv_data.toarray(), columns=cv.get_feature_names())\n",
    "dtm.index = df.index\n",
    "dtm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's pickle our dataframes\n",
    "dtm.to_pickle('dtm.pkl')\n",
    "\n",
    "# and our CountVectorizer object\n",
    "with open(\"cv.pkl\", \"wb\") as f:\n",
    "    pickle.dump(cv, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Top words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = dtm.transpose()\n",
    "data.head(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the most common words used by the 10-K files\n",
    "top_words = {}\n",
    "\n",
    "for firm in data.columns:\n",
    "    top = data[firm].sort_values(ascending=False).head(30)\n",
    "    top_words[firm] = list(zip(top.index, top.values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print top words for each company\n",
    "for firm, words in top_words.items():\n",
    "    print(firm)\n",
    "    print(','.join([word for word, count in words[0:14]]))\n",
    "    print('------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a bag of contrainning words from BLM (2015)\n",
    "constraining_words = []\n",
    "\n",
    "with open('words_from_pdf.txt', 'r') as rf:\n",
    "        lines = rf.read().splitlines() # readlines() create a newline character \"\\n\" each line\n",
    "        for line in lines:\n",
    "            words = line.split(sep=' ')\n",
    "            for word in words:\n",
    "                constraining_words.append(word)\n",
    "\n",
    "# sort words alphabetically               \n",
    "constraining_words.sort()\n",
    "\n",
    "# You can write to a local file of course    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "constraining_words"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
